{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_TOKEN_WARNING\"] = \"1\"\n",
    "\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "disable_progress_bar()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import random  # Import random module for shuffling data\n",
    "import torch  # Import PyTorch for tensor computations\n",
    "import torch.nn as nn  # Import neural network modules\n",
    "import torch.optim as optim  # Import optimization algorithms\n",
    "from torch.utils.data import DataLoader, Dataset  # Import PyTorch dataset utilities\n",
    "from datasets import load_dataset  # Import function to load datasets\n",
    "from transformers import AutoTokenizer  # Import tokenizer for text processing\n",
    "from torchinfo import summary  # Import module for model summary\n",
    "\n",
    "print(\"Import completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data and Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset from the Hugging Face datasets library\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Load the tokenizer for BERT (bert-base-uncased) to process text data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define tokenization function\n",
    "def preprocess_text(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes input text using the BERT tokenizer and applies padding/truncation.\n",
    "    \"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "dataset = dataset.map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(split, num_samples=5000):\n",
    "    \"\"\"\n",
    "    Prepares a subset of the dataset by selecting random samples and converting them into tensors.\n",
    "    \"\"\"\n",
    "    dataset_split = dataset[split]  # Select the dataset split (train or test)\n",
    "    indices = list(range(len(dataset_split)))  # Create a list of indices for dataset\n",
    "    random.seed(42)  # Set seed for reproducibility\n",
    "    random.shuffle(indices)  # Shuffle dataset indices randomly\n",
    "    subset_indices = indices[:num_samples]  # Select a subset of the data\n",
    "    subset = [dataset_split[i] for i in subset_indices]  # Retrieve selected samples\n",
    "    processed_data = [\n",
    "        (\n",
    "            torch.tensor(ex[\"input_ids\"], dtype=torch.long),  # Convert tokenized input IDs into a PyTorch tensor\n",
    "            torch.tensor(ex[\"label\"], dtype=torch.long)  # Convert label (0 or 1) into a PyTorch tensor\n",
    "        ) for ex in subset  # Iterate over each selected sample in the subset\n",
    "    ]\n",
    "    return processed_data  # Return processed dataset as list of tuples\n",
    "\n",
    "# Prepare train and test datasets with a subset of 5000 samples each\n",
    "train_data = prepare_data(\"train\", 5000)  # Process training data\n",
    "test_data = prepare_data(\"test\", 5000)  # Process testing data\n",
    "\n",
    "# Define custom dataset class\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with the provided data.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample at the specified index.\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset objects for training and testing\n",
    "train_dataset = IMDBDataset(train_data)  # Initialize training dataset\n",
    "test_dataset = IMDBDataset(test_data)  # Initialize testing dataset\n",
    "\n",
    "# Define batch size for training and testing\n",
    "BATCH_SIZE = 32  # Set batch size\n",
    "\n",
    "# Create DataLoaders for efficient data loading during training and evaluation\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)  # DataLoader for training\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)  # DataLoader for testing\n",
    "\n",
    "print(\"Data preparation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define the TextCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (embedding): Embedding(30522, 128)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 128), stride=(1, 1))\n",
       "    (1): Conv2d(1, 10, kernel_size=(4, 128), stride=(1, 1))\n",
       "    (2): Conv2d(1, 10, kernel_size=(5, 128), stride=(1, 1))\n",
       "  )\n",
       "  (fc): Linear(in_features=30, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model hyperparameters\n",
    "VOCAB_SIZE = tokenizer.vocab_size  # Get vocabulary size from tokenizer\n",
    "EMBEDDING_DIM = 128  # Dimension of word embeddings\n",
    "NUM_CLASSES = 2  # Number of output classes (positive/negative sentiment)\n",
    "FILTER_SIZES = [3, 4, 5]  # Different filter sizes for convolution layers\n",
    "NUM_FILTERS = 10  # Number of filters per convolutional layer\n",
    "NUM_EPOCHS = 3  # Number of training epochs\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes, filter_sizes, num_filters):\n",
    "        \"\"\"\n",
    "        Initializes the TextCNN model with embedding, convolutional, and fully connected layers.\n",
    "        It does not process input data but sets up the model structure.\n",
    "        \"\"\"\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding layer\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (fs, embedding_dim)) for fs in filter_sizes  # Apply different filter sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)  # Fully connected layer\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define how the input data flows through the network.\n",
    "        It applies the layers defined in __init__() to the input and computes the output.\n",
    "        This is where the actual computation (like embedding lookup, convolution, activation functions,\n",
    "        and classification) happens when the model is used.\n",
    "        \"\"\"\n",
    "        x = self.embedding(x).unsqueeze(1)  # Convert input into embeddings and add a channel dimension\n",
    "        x = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]  # Apply convolution layers\n",
    "        x = [torch.max(pool, dim=2)[0] for pool in x]  # Apply max pooling\n",
    "        x = torch.cat(x, dim=1)  # Concatenate feature maps\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        return self.fc(x)  # Output layer\n",
    "\n",
    "# Initialize the TextCNN model with predefined parameters\n",
    "model = TextCNN(VOCAB_SIZE, EMBEDDING_DIM, NUM_CLASSES, FILTER_SIZES, NUM_FILTERS)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check device availability\n",
    "model.to(device)  # Move model to selected device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train and Evaluate The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.7829\n",
      "Epoch 2/3, Loss: 0.6722\n",
      "Epoch 3/3, Loss: 0.6306\n",
      "Test Accuracy: 0.7124\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer for training\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # Adam optimizer with specified learning rate\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Trains the model for a specified number of epochs.\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(NUM_EPOCHS):  # Loop through epochs\n",
    "        total_loss = 0  # Initialize total loss\n",
    "        for batch in train_loader:  # Iterate over training batches\n",
    "            inputs, labels = batch  # Unpack input features and labels\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the correct device\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            outputs = model(inputs)  # Forward pass through the model\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model weights\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")  # Print average loss per epoch\n",
    "\n",
    "def evaluate_model():\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0  # Initialize correct predictions count\n",
    "    total = 0  # Initialize total sample count\n",
    "    with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "        for batch in test_loader:  # Iterate over test batches\n",
    "            inputs, labels = batch  # Unpack inputs and labels\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the correct device\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            predictions = torch.argmax(outputs, dim=1)  # Get predicted class labels\n",
    "            correct += (predictions == labels).sum().item()  # Count correct predictions\n",
    "            total += labels.size(0)  # Count total samples\n",
    "    print(f\"Test Accuracy: {correct / total:.4f}\")  # Print test accuracy\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_model()\n",
    "evaluate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Improving Model Performance\n",
    "\n",
    "Some hints:\n",
    "* Hyperparameter tuning, such as increasing or decreasing batch size\n",
    "* Experiment with different filter sizes and number of filters\n",
    "* Train for more epochs and observe if performance improves or overfits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
