{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_TOKEN_WARNING\"] = \"1\"\n",
    "\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "disable_progress_bar()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import random  # Import random module for shuffling data\n",
    "import torch  # Import PyTorch for tensor computations\n",
    "import torch.nn as nn  # Import neural network modules\n",
    "import torch.optim as optim  # Import optimization algorithms\n",
    "from torch.utils.data import DataLoader, Dataset  # Import PyTorch dataset utilities\n",
    "from transformers import AutoTokenizer  # Import tokenizer for text processing\n",
    "from torchinfo import summary  # Import module for model summary\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Import completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data and Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the file you'd like to load\n",
    "file_path = \"../datasets/WELFake_Dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.dropna(subset=[\"title\", \"text\", \"label\"])\n",
    "df[\"title\"] = df[\"title\"].astype(str)\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "MAX_SAMPLES = 10000\n",
    "df = df[:MAX_SAMPLES] # Limit dataset for testing\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the texts in the dataframe\n",
    "tokenized = tokenizer(\n",
    "    list(df[\"title\"]),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Extract tensors\n",
    "input_ids = tokenized[\"input_ids\"]\n",
    "labels = torch.tensor(df[\"label\"].values, dtype=torch.long)\n",
    "\n",
    "# Combine inputs and labels\n",
    "data = list(zip(input_ids, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "split_idx = int(0.8 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "test_data = data[split_idx:]\n",
    "\n",
    "# Define custom dataset class\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with the provided data.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample at the specified index.\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset objects for training and testing\n",
    "train_dataset = FakeNewsDataset(train_data)  # Initialize training dataset\n",
    "test_dataset = FakeNewsDataset(test_data)  # Initialize testing dataset\n",
    "\n",
    "# Define batch size for training and testing\n",
    "BATCH_SIZE = 32  # Set batch size\n",
    "\n",
    "# Create DataLoaders for efficient data loading during training and evaluation\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)  # DataLoader for training\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)  # DataLoader for testing\n",
    "\n",
    "print(\"Data preparation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define the TextCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (embedding): Embedding(30522, 128)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 12, kernel_size=(3, 128), stride=(1, 1))\n",
       "    (1): Conv2d(1, 12, kernel_size=(5, 128), stride=(1, 1))\n",
       "    (2): Conv2d(1, 12, kernel_size=(7, 128), stride=(1, 1))\n",
       "  )\n",
       "  (fc): Linear(in_features=36, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model hyperparameters\n",
    "VOCAB_SIZE = tokenizer.vocab_size  # Get vocabulary size from tokenizer\n",
    "EMBEDDING_DIM = 128  # Dimension of word embeddings\n",
    "NUM_CLASSES = 2  # Number of output classes (real/fake news)\n",
    "FILTER_SIZES = [3, 5, 7]  # Different filter sizes for convolution layers\n",
    "NUM_FILTERS = 12  # Number of filters per convolutional layer\n",
    "NUM_EPOCHS = 10  # Number of training epochs\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes, filter_sizes, num_filters):\n",
    "        \"\"\"\n",
    "        Initializes the TextCNN model with embedding, convolutional, and fully connected layers.\n",
    "        It does not process input data but sets up the model structure.\n",
    "        \"\"\"\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding layer\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (fs, embedding_dim)) for fs in filter_sizes  # Apply different filter sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)  # Fully connected layer\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define how the input data flows through the network.\n",
    "        It applies the layers defined in __init__() to the input and computes the output.\n",
    "        This is where the actual computation (like embedding lookup, convolution, activation functions,\n",
    "        and classification) happens when the model is used.\n",
    "        \"\"\"\n",
    "        x = self.embedding(x).unsqueeze(1)  # Convert input into embeddings and add a channel dimension\n",
    "        x = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]  # Apply convolution layers\n",
    "        x = [torch.max(pool, dim=2)[0] for pool in x]  # Apply max pooling\n",
    "        x = torch.cat(x, dim=1)  # Concatenate feature maps\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        return self.fc(x)  # Output layer\n",
    "\n",
    "# Initialize the TextCNN model with predefined parameters\n",
    "model = TextCNN(VOCAB_SIZE, EMBEDDING_DIM, NUM_CLASSES, FILTER_SIZES, NUM_FILTERS)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check device availability\n",
    "model.to(device)  # Move model to selected device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train and Evaluate The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4720\n",
      "Epoch 2/10, Loss: 0.3281\n",
      "Epoch 3/10, Loss: 0.2597\n",
      "Epoch 4/10, Loss: 0.2074\n",
      "Epoch 5/10, Loss: 0.1649\n",
      "Epoch 6/10, Loss: 0.1323\n",
      "Epoch 7/10, Loss: 0.1011\n",
      "Epoch 8/10, Loss: 0.0887\n",
      "Epoch 9/10, Loss: 0.0672\n",
      "Epoch 10/10, Loss: 0.0547\n",
      "Test Accuracy: 0.9015\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer for training\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # Adam optimizer with specified learning rate\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Trains the model for a specified number of epochs.\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(NUM_EPOCHS):  # Loop through epochs\n",
    "        total_loss = 0  # Initialize total loss\n",
    "        for batch in train_loader:  # Iterate over training batches\n",
    "            inputs, labels = batch  # Unpack input features and labels\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the correct device\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            outputs = model(inputs)  # Forward pass through the model\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model weights\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")  # Print average loss per epoch\n",
    "\n",
    "def evaluate_model():\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0  # Initialize correct predictions count\n",
    "    total = 0  # Initialize total sample count\n",
    "    with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "        for batch in test_loader:  # Iterate over test batches\n",
    "            inputs, labels = batch  # Unpack inputs and labels\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the correct device\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            predictions = torch.argmax(outputs, dim=1)  # Get predicted class labels\n",
    "            correct += (predictions == labels).sum().item()  # Count correct predictions\n",
    "            total += labels.size(0)  # Count total samples\n",
    "    print(f\"Test Accuracy: {correct / total:.4f}\")  # Print test accuracy\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_model()\n",
    "evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 5 MIND BLOWING ISSUES VOTING AMERICANS REALIZED THIS ELECTION\n",
      "This is fake news!\n",
      "Confidence scores: Real: 0.0008%, Fake: 0.9992%\n",
      "\n",
      "How US election fraud claims changed as Trump won\n",
      "This is fake news!\n",
      "Confidence scores: Real: 0.068%, Fake: 0.932%\n",
      "\n",
      "'Google AI presented my April Fools' story as real news'\n",
      "This is true!\n",
      "Confidence scores: Real: 0.9985%, Fake: 0.0015%\n",
      "\n",
      "Cwmbran's roundabouts make the Guinness Book of World Records!\n",
      "This is fake news!\n",
      "Confidence scores: Real: 0.0053%, Fake: 0.9947%\n",
      "\n",
      "A law enforcement sniper assigned to former President Donald Trump’s rally Saturday in Butler, Pennsylvania, says the head of the Secret Service ordered him not to shoot the suspect accused of attempting to assassinate Trump.\n",
      "This is fake news!\n",
      "Confidence scores: Real: 0.0166%, Fake: 0.9834%\n",
      "\n",
      "A vaccination that is like 38 different vaccines and it looks like it’s meant for a horse” is being given to babies, making them start to change radically, former President and Republican presidential nominee Donald Trump in a phone call to independent presidential candidate Robert F. Kennedy Jr.\n",
      "This is fake news!\n",
      "Confidence scores: Real: 0.0%, Fake: 1.0%\n",
      "\n",
      "A photo taken on Monday shows former President Donald Trump with no damage to his right ear, contrary to reports that it was injured in an attempted assassination on Saturday.\n",
      "This is true!\n",
      "Confidence scores: Real: 0.9437%, Fake: 0.0563%\n",
      "\n",
      "Starbucks is sponsoring the Republican National Convention in Milwaukee.\n",
      "This is true!\n",
      "Confidence scores: Real: 0.6008%, Fake: 0.3992%\n",
      "\n",
      "Trump ambushes S African leader with claim of Afrikaners being 'persecuted'\n",
      "This is true!\n",
      "Confidence scores: Real: 0.9945%, Fake: 0.0055%\n",
      "\n",
      "How a joke about rice cost a Japan cabinet minister his job\n",
      "This is fake news!\n",
      "Confidence scores: Real: 0.0029%, Fake: 0.9971%\n",
      "\n",
      "Ukrainian ex-top official shot dead outside Madrid school\n",
      "This is true!\n",
      "Confidence scores: Real: 0.8739%, Fake: 0.1261%\n",
      "\n",
      "Kneecap member charged with terror offence\n",
      "This is true!\n",
      "Confidence scores: Real: 0.7133%, Fake: 0.2867%\n",
      "\n",
      "Roof of historic Ming Dynasty tower collapses in China\n",
      "This is true!\n",
      "Confidence scores: Real: 0.8263%, Fake: 0.1737%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer, device):\n",
    "    model.eval()  # Set model to eval mode\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the input string\n",
    "        tokens = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokens[\"input_ids\"].to(device)  # Move to same device as model\n",
    "\n",
    "        # Get model prediction\n",
    "        output = model(input_ids)\n",
    "        \n",
    "        return output\n",
    "\n",
    "texts = [\n",
    "    \"TOP 5 MIND BLOWING ISSUES VOTING AMERICANS REALIZED THIS ELECTION\",\n",
    "    \"How US election fraud claims changed as Trump won\",\n",
    "    \"'Google AI presented my April Fools' story as real news'\",\n",
    "    \"Cwmbran's roundabouts make the Guinness Book of World Records!\",\n",
    "    \"A law enforcement sniper assigned to former President Donald Trump’s rally Saturday in Butler, Pennsylvania, says the head of the Secret Service ordered him not to shoot the suspect accused of attempting to assassinate Trump.\",\n",
    "    \"A vaccination that is like 38 different vaccines and it looks like it’s meant for a horse” is being given to babies, making them start to change radically, former President and Republican presidential nominee Donald Trump in a phone call to independent presidential candidate Robert F. Kennedy Jr.\",\n",
    "    \"A photo taken on Monday shows former President Donald Trump with no damage to his right ear, contrary to reports that it was injured in an attempted assassination on Saturday.\",\n",
    "    \"Starbucks is sponsoring the Republican National Convention in Milwaukee.\",\n",
    "    \n",
    "    \"Trump ambushes S African leader with claim of Afrikaners being 'persecuted'\",\n",
    "    \"How a joke about rice cost a Japan cabinet minister his job\",\n",
    "    \"Ukrainian ex-top official shot dead outside Madrid school\",\n",
    "    \"Kneecap member charged with terror offence\",\n",
    "    \"Roof of historic Ming Dynasty tower collapses in China\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    logits = predict_sentiment(\n",
    "        text = text,\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        device = device\n",
    "    )\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    prediction = torch.argmax(logits, dim=1)\n",
    "    print()\n",
    "    print(text)\n",
    "    print(\"This is fake news!\" if prediction == 1 else \"This is true!\")\n",
    "    print(f\"Confidence scores: Real: {round(float(probs[0][0]), 4)}%, Fake: {round(float(probs[0][1]), 4)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Improving Model Performance\n",
    "\n",
    "Some hints:\n",
    "* Hyperparameter tuning, such as increasing or decreasing batch size\n",
    "* Experiment with different filter sizes and number of filters\n",
    "* Train for more epochs and observe if performance improves or overfits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
